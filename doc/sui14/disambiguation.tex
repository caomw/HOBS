\section{Disambiguation Techniques}
\label{sec:disamb-techn}

In our previous section we have described how we design and choose the COTS components for our prototype. However, for area selection technique, when multiple targets are within the range, the disambiguation is necessary. We could just leave this as a manual operation to the user; however, from previous studies \cite{Chen:EECS-2013-200} we have found that this becomes the bottleneck for seamless interaction in many cases. Therefore, we consider the problem of simplifying disambiguation in physical targeting contexts by incorporating multiple sources of sensors. 

The first and most straightforward way is to use the signal strength of area selection technique. For IR, we have added IR light-to-voltage converter to read the absolute IR intensity. The second class of sensors are those available on wearable devices. Google Glass' motion sensors are the ones that we have used in our study. The automatic disambiguation will definitely save time, but leaving the option of manual switching targets will still be useful. But this doesn't necessarily mean the naive list browsing technique. We consider how the IR and motion sensors can helps us build a smarter list -- adjacent map -- when manual switching is performed. We talk about each technique in the following few subsections. 

\subsection{Using IR Intensity}
\label{sec:using-ir-intensity}

From the IR datasheet\footnote{https://www.sparkfun.com/datasheets/Components/LED/YSL-R531FR2C-F1.pdf}, for a directional IR emitter, the illuminance at the receiver falls off as the receiver is positioned at an angle from the emitter. Therefore, given multiple targets that are equidistant from the user, the one straight most directly in the line of sight will read the highest intensity values. However, illuminance could also drop off as $\frac{1}{d^2}$ where $d$ is the distance between emitter and receiver. So a closer targets will read higher intensity values. This introduces problems for two targets which are not equidistant from the user, since the closer one will always read a higher value. To overcome this problem, we utilize the fact that the user might move his head a little bit and the changes in the intensity readings will give us such information. We put a higher weight on those with a higher intensity reading gains.

\subsection{Using Glass Sensors}
\label{sec:using-glass-sensors}
The second disambiguation technique is to use the sensors on Google Glass. As we discussed before, Google Glass not only provides us with the head-up display, but also many other sensors. For head orientation, we uses the rotation vector sensor provided by Android API. Such orientation provides the relative movement of user's head. However, without the absolute map, we cannot easily map the readings into targets. In this case, the combination of IR directions provide an anchor direction and the Glass application calibrates its estimation of orientation. From a successful targeting event, we construct the adjacent map during the learning phase and use the model to predict user's targeting if we detect multiple IR readings and also user's head movement. 

\subsection{Manual Disambiguation}
\label{sec:manu-disamb}

For cases where the ambient IR noise is too high, and the environment is too dense, or when our disambiguation fails, we always enable the manual disambiguation option to the users. This is achieved in a way similar to mouse click. When the user single click, we only display the current highlighted targets, when the user click again (a double clicke event), we confirm the selection. Therefore, before the second click even happens, the user is always free to change the target by swiping forward or backward.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "uist14"
%%% End: 
