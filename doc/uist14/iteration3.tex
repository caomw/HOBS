\section{Iteration 3: Head motion for disambiguation}
\label{sec:iteration-3:-head}
\subsection{Problem Statement and Goal}
Though our {\em Valued IR} system already performs well (with 3.71 seconds on average in a relative dense environment), head-orientation information is only used during the {\em coarse selection} stage. In the {\em fine selection} phase, we rely solely on the near-eye display. This has two problems: First, we are projecting a high dimension distribution to a 1-dimension list. Second, this requires the user to switch their focus back and forth between the physical world and the display. This issue motivated us to design an approach that also utilizes orientation information during the {\em fine selection} stage, so that there would be a more direct mapping between the user's gestures and target selection.

In our third iteration, we ask the question ``can we build a system purely using head orientation and visual feedback from the environment for target selection?''

\subsection{Technique}
The motion sensors (including gyroscope, magnetometer and accelerometer) on the head-worn device can help us determine the orientation of users' head. However, this absolute orientation cannot be directly used in any indoor environment as the user may change his location \achal{Should we use gender neutral language?}. However, as long as the targets are spread around the periphery, their relative orientation doesn't change. This key observation inspires us to investigate a hybrid solution with IR and sensors on Glass.

There are two phases to this technique: learning the relative orientations and intelligently suggesting targets during disambiguation.

In the first stage, the user orients his head towards each device, and the system saves the absolute orientations of each device. Once the user has added all devices, these orientations are saved for future use. It is still possible to add new devices at any point without running the entire calibration step, as the system will simply store the absolute orientation of the new device.

During the second stage, when the user wants to use his head-motion for disambiguation, he will put his finger on the touchpad to enter a quasi-mode.  This gesture is inspired by observing users who tend to adjust their glasses for closer inspection.

In this mode, only one device has a solid indicator light on at a time. The user can then tilt in the direction of the device he wishes to switch to. We calculate the direction of movement using a low-passed history of orientations, and look for devices that lay in that direction. Once a new device is selected, the frame of reference changes, and the user can repeat the process. Note that in reality, this is a completely smooth operation: it is possible to switch between multiple devices in one continuous motion. This is achieved by automatically adjusting the frame of reference to the currently lit device, but also creating a ``sticky" threshold so that the user is not constantly jumping between devices.

\achal{Not sure if there should be a conclusion of some sort here.}

\subsection{Evaluation}
\ben{We then need evaluation results here, quantitative and qualitative.}



% %!TEX root = uist14.tex
% \section{Iteration 3: Orientation-based refinement}
% \bjoern{While taking IR intensity into account further reduced the need for manual refinement and increased performance, the UI navigation scheme is problematic - it is not spatially related in any meaningful way to the layout of targets in a room. A better interaction technique would respect that ordering. For example, when refinement is needed, tilting the head slightly to the right could select the target that's adjacent to the right of the currently selected target.

% Such spatial navigation requires knowledge about the layout of targets in the environment. However, one of the strengths of our technique so far is that it does not require any map ahead of time. To enable some spatial navigation, we introduce a final iteration in which we build up a spatial data structure by demonstration (i.e., the user looks around the room) and then leverage that data structure during the refinement step of our interaction.}

% \bjoern{This technique is based on the assumption that users will generally select targets in indoor environments where targets are spread around the periphery. These assumptions enable us to use orientation data without knowing the user's absolute position.}

% \subsection{Implementation}
% \bjoern{give implementation details}

% \subsection{Informal User Feedback}
% \bjoern{We informally evaluate this technique with N users: ...}

 
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "uist14"
%%% End: 
