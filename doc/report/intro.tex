\section{Introduction}
\label{sec:introduction}

The vision of smart-home and smart office is being realized recently with the development of radio technology, sensors, and actuators. Nowadays, you can buy the basic components online \cite{SmartHome, NinjaBlocks} and many hobbyists have already created their own smart environment (see \cite{BRAD} as an example). Though various challenges are ahead in the field to build a more robust, scalable, energy-efficient, and user-friendly systems, the general trend of having smart appliances around us is foreseeable. Those smart appliances each can be controlled separately (like what is existing now for television and heater), however, with the prevalence of smartphones, many systems \cite{SmartThings, Lockitron} are being designed as applications on mobile platforms so that the smartphone act as an universal bridge between human and appliances.

Admittedly, people have been used to accessing the Internet, conversing with friends, and even dealing with online business through their smartphones. And putting the control function of physical device into smartphones can significantly lowers the barrier for ordinary users. Also users can get other benefits by using smartphones -- networking,  display, gesture input, and the increasing computability. 

However, we argue that traditional graphical user interface (GUI) on mobile platforms still imposes difficulty in interacting with real-world objects. Imagine when we have all the control panel integrated into iPhone, users then need to browse a list of possible icons and select the right one. Even if you can create widget to simplify the browsing overhead, such as on Android platform, the problem still holds since squeezing physically distributed objects onto a few-inch screen is not an intuitive mapping. In an ideal system, the user should be able to express their interest of interaction in a more direct way. In addition, taking out the phones (including locating them, turn on the screen, perhaps entering the passcode, and open the application) is relatively time-consuming. According to a study in \cite{Ashbrook:2008:QIM:1357054.1357092}, 4 seconds are required on average just to get a phone out of the pocket or hip holster, not to mention the time to choose the target and initiate interaction. This project is trying to propose a new way of interacting with physical objects without losing attention, essentially solving the ``addressing'' problem in interacting with physical devices around.

Attention Aware System (AAS) \cite{horvitz2003models} has been studied for a while in understanding how users' attention affects their interaction, and how system should affect or manage users' attention. This comes from some observations in human-human interaction. For people to direct interact with physical objects, their attention can be a strong indicator of their interest. When staring at the television, the user is more likely to change channels or do other TV related controls rather than turn off the heater. If they do want to control the heater, they will need to express this explicitly. Discussion of understanding human's thoughts through brain-computer interface is beyond the scope of this work. We utilize users' ``gesture'' to know their interest for interaction.

Some direct gesture include looking, pointing and reaching, and there is one interesting paper \cite{Merrill:2007:ALP:1758156.1758158} which put sensors to users' ears and fingers to detect their intention. And this information is fed back to the system to assist user in browsing physical objects. Our approach was inspired by this paper. We utilize sensors on users' body, however, we enable the direct interaction rather than just passive sensing. The other major difference is that we choose to use glass form factor for such attention expression. Firstly, line of sight is easier to detect on glasses, rather than earpiece. And this is also more intuitive to users that when they are looking at something, they might be interested in that, and this information is captured by a on-glass sensor. Second, with the release of Google Glass, we believe that it could serve as the enabling hardware platform. Though in this paper, we will still present some of our thoughts on ``what if we have an advanced glass and various sensors'', we focus most of the discussion on our prototype which uses IR to express users' interst, XBee radio for device communication, and a slider for users' gesture. We recognize that there is a great room of improvement for making the system more robust and the device more compact. But for this semester project, our goal (for us) is to explore the possibility of enabling direct interaction with physical objects, learn how to prototype novel interaction device on our own, and apply what we have learned and discussed in class about human computer interaction into a real project.

The contribution in this work is to propose a new way of interacting with physical device and simplify every task. Thoughts and discussion on this topic might also be beneficial to researchers on similar work.

The rest of this paper is structured as follows. We will first give an overview of some related projects, and discuss the similarities and different of our work from theirs. Then we present our approach in detail about the system design and implementation in the following two sections respectively. Evaluations and in-depth discussions will be provided before we concludes this paper and talk about potential future work.

% ref didn't work in SIGCHI template
% The rest of this paper is structured as follows. In Sec.\,\ref{sec:related-work}, we will give an overview of some related projects. Then we present our approach in detail about the system design and implementation in Sec.\,\ref{sec:design} and Sec.\,\ref{sec:implementation} respectively. Then Sec.\,\ref{sec:evaluation} shows our evaluation results and we provide an in-depth discussion (Sec.\,\ref{sec:discussion}) about different kinds of interaction since not all are suitable for our system. Sec.\,\ref{sec:conclusion} concludes this paper.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
