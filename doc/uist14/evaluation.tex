\section{Evaluation}
\label{sec:evaluation}

\ben{still thinking how to proceed.}
In this section, we evaluate our targeting system by benchmarking its performance with an user study. We first describe our design of the physical targeting study, followed by the results in different scenarios.

\subsection{Apparatus}
We deployed 10 wireless nodes in an indoor environment at various distance and density. Then the participant is asked to stand in a fixed position in the room and look down before the targeting request is made. We randomly generate a target and highlight it using a yellow LED, and then measure the amount of the time for a user to achieve a targeting transaction. 

\subsection{Methodology}
\label{sec:methodology}

We break down the time for a complete target acquisition to the following pieces:
$t_{total}=t_{locate}+t_{reorient}+t_{disambiguate}+t_{tap}$
And during the study, we video-taped the participants' behavior and measure each pieces of the time. The experiment setup is similar to a Fitts' Law target acquisition task. We highlight one of the targets and ask the user to make the selection.

In our previuos studies, we have made a comparison of using IR and Glass List UI. There we reach an conclusion that once the targets numbers have exceeded a certain value (6 in a single room), then list selection (using Google Glass List UI) will be worse than IR targeting. However, the average results there comes from cases where disambiguation is not needed and needed. And disambiguation is a time-consuming part. So in this paper we focus more study on the disambiguation techniques.


\subsection{IR intensity based}
We evaluate how much gain we can get by performing an automatated disambiguation if the IR intensity reading is available. We compare against the case where no IR intensity is available. 

\subsection{IR intensity together with Google Glass}

\subsection{Manual disambiguation}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "uist14"
%%% End: 
