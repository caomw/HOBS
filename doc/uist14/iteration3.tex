\section{Iteration 3: Head motion for disambiguation}
\label{sec:iteration-3:-head}
\subsection{Problem Statement and Goal}
Though our {\em Valued IR} system is already performing well (with 3.71 seconds on average in a relative dense environment), head-orientation information is only used during the {\em coarse selection} stage. In the {\em fine selection} part, we rely on the near-eye display. This has two problems: First, we are projecting a high dimension distribution to a 1-dimension list. Second, this requires the user to switch their focus back and forth between the physical world and the near-eye display. Therefore, it motivates us design an approach that also utilize head orientation during the {\em fine selection} stage, and that there can be a more direct mapping between the user's gestures and target traversal. 

In our third iteration, we ask the question ``can we build a system purely using head orientation and visual feedback from the environment for target selection?''

\subsection{Technique}
The sensors (including gyroscope, magnetometer and accelerometer) on the head-worn device can help us determine the orientation of users' head. However, such absolute orientation cannot be directly used in any indoor environment because the user might change his location. However, as long as the targets are spread around the periphery, their relative orientation doesn't change. This key observation inspires us to investigate a hybrid solution with IR and sensors on Glass.

Before we describe how this new technique works. The relative orientation information -- {\em adjacent map} -- has to be learned before we can use it. Such a learning process can either be performed in advance, or through demonstration. \ben{need better elaboration here}.

In this hybrid approach, in additional to use IR to reduce the chance of disambiguation and prepare a better list, the IR information helps us identify a base point. When the user wants to use his head-motion for disambiguation, he will put his finger on the touchpad to enter a quasi-mode. Such a gesture comes from a real-world metaphor that people tend to adjust their glasses for a closer inspection. In such quasi-mode, we monitor the changes of his head orientation, and infer the intended next targets according to some algorithm \ben{haven't described yet}.

\subsection{Evaluation}
\ben{We then need evaluation results here, quantitative and qualitative.}



% %!TEX root = uist14.tex
% \section{Iteration 3: Orientation-based refinement}
% \bjoern{While taking IR intensity into account further reduced the need for manual refinement and increased performance, the UI navigation scheme is problematic - it is not spatially related in any meaningful way to the layout of targets in a room. A better interaction technique would respect that ordering. For example, when refinement is needed, tilting the head slightly to the right could select the target that's adjacent to the right of the currently selected target.

% Such spatial navigation requires knowledge about the layout of targets in the environment. However, one of the strengths of our technique so far is that it does not require any map ahead of time. To enable some spatial navigation, we introduce a final iteration in which we build up a spatial data structure by demonstration (i.e., the user looks around the room) and then leverage that data structure during the refinement step of our interaction.}

% \bjoern{This technique is based on the assumption that users will generally select targets in indoor environments where targets are spread around the periphery. These assumptions enable us to use orientation data without knowing the user's absolute position.}

% \subsection{Implementation}
% \bjoern{give implementation details}

% \subsection{Informal User Feedback}
% \bjoern{We informally evaluate this technique with N users: ...}

 
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "uist14"
%%% End: 
