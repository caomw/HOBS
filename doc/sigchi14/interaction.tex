\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/stepbystep.png}
\caption{(A) Glass periodically sends out infrared signal aligned with the direction user is facing. (B) Device toggles red led as visual feedback when receiving valid signals. (C) User taps to connect with responding client}
\label{fig:interaction}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/stepbystep_multi.png}
\caption{(A) Multiple clients are within range. (B) All responding clients lights up blue led while the current hovered one blinks. (C) swipe the touchpad to traverse between responding devices. }
\label{fig:interaction_multi}
\end{figure}

\section{Initiating Interaction through Attention}
This section describes the design goals of our systems and their realization in particular interaction techniques.

\subsection{Design Goals}
Our work is motivated by the following design goals that leverage opportunities of head-worn computing, but also acknowledge potential challenged:

{\bf Leverage visual attention:} Take advantage of the fact that visual attention can express intention - initiate interaction based on where a user is already looking. 

{\bf Provide immediate feedback about selection targets in the environment:} While a near-eye display can push information to the user, users don't always want to control an object simply because they are looking at it (a problem known in gaze-based interaction as the Midas touch). A calmer~\cite{weiser_coming_1997} approach is to locate visual feedback about selection targets in the environment, to prevent distraction and interruption. Such feedback should be delivered instantaneously, while users look around a room.

{\bf Offer flexible orientation after initiating interaction:} After initiating attention through head orientation, enable the user to reorient their head or body position during the remaining interaction to prevent neck strain.

{\bf Offer efficient ways to disambiguate orientation input:} It may not always be possible to identify a unique target device based on a user's attention and orientation. Offer ways to supplement orientation-based interaction with screen-based interaction to provide disambiguation information.

These design goals find their expression in the following interaction model:

\subsection{Interaction Flow}
{\bf Look:} Users select a target device by looking in its general direction.
Glass periodically sends a device id through its IR emitter analogous to Patel's approach~\cite{patel_2-way_2003}. Target appliances have IR receivers and offer immediate visual feedback by toggling a red LED whenever a valid id is received (Figure~\ref{fig:interaction}B). This enables {\em scanning} the environment with ones gaze to see which devices can be controlled.

{\bf Initiate:} Users confirm their desire to connect to a device by tapping on the Glass touchpad. The next section on disambiguation deals with cases in which multiple devices received valid IR signals. At this point, all further communication switches over to the 802.15.4 wireless network so that line of sight to the target is no longer needed.

{\bf Control:} Glass displays a user interface for parameters of the chosen device. The interface is controlled with the temple-mounted touchpad through the following gesture set: tapping toggles discrete parameters; single finger swipe changes between available parameters; double finger swipe adjusts continuous parameters. This scheme was chosen because the touchpad is only comfortably operable in the coronal plane (front to back) but not in the saggital plane (up and down). 
Control commands are sent over XBee radios.

{\bf Disengagement:} Users stay connected to the last selected device up to a timeout period. During that period, users can disengage through down swipes.

\subsection{Disambiguation}
Head orientation only indicates a general area of visual interest. It does not necessarily match gaze orientation as extra-ocular muscles can move the eyes. The IR beam of our device also has a certain spread (see next section). In an environment dense with potential targets, multiple targets could be within range. Users can tell when multiple feedback LEDs in the environment illuminate (Figure~\ref{fig:interaction_multi}A). To disambiguate, users can either move to adjust their head position, or, alternatively, call up a disambiguation dialog on the Glass display. The dialog presents a list filtered to only those devices that are within IR range, whlie devices also lights up blue led as visual cues. Users navigate the list using the touchpad (Figure~\ref{fig:interaction_multi}B), and then continue their interaction as described above.
