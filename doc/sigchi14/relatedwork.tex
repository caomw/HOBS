\section{Related Work}
Relevant prior work exists in the areas of remote control of physical appliances and evaluations of pointing in physical space. We discuss each in turn.

\subsection{Remote Control of Physical Appliances}
A number of researchers propose to use different handheld devices to control appliances~\cite{beigl_point_1999,patel_2-way_2003,wilson_xwand:_2003,schmidt_picontrol:_2012} or exchange other information with smart infrastructure like reading sensor netowrk data~\cite{lifton_tricorder:_2007,mittal_ubicorder:_2011,costanza_sensortune:_2010}. Key design decisions are the method by which a target device is selected; and the method by which it is then later controlled or configured.

Several techniques use laser pointers to directly point at objects of interest. This has the advantage of also providing immediate visual feedback to the user in the shape of the laser dot. Beigl's early AIDA handheld combines laser pointing for selection with IR communication to exchange commands~\cite{beigl_point_1999}. Patel extends this technique by modulating the laser light to communicate the controllers identity~\cite{patel_2-way_2003}. Kemp et al. use a laser pointer to indicate to robots which item to pick up in a room~\cite{kemp_point-and-click_2008}. 

The XWand~\cite{wilson_xwand:_2003} determines its absolute position and orientation and uses a virtual room model to select target devices. Position is determined through two ceiling-mounted cameras; orientation is determined using a built-in IMU. Users can employ physical gestures or utter speech commands to control selected devices. This technique requires room instrumentation and an up-to-date virtual model of device locations. The Tricorder~\cite{lifton_tricorder:_2007} uses IMU orientation coupled with room-localization based on received signal strength indicators (RSSI) to estimate what a user is pointing at.

Handheld projectors can both display a user interface in space and communicate control information optically, e.g., by encoding information temporally (using Gray codes in PiControl~\cite{schmidt_picontrol:_2012}) or spatially (using QR codes in the infrared spectrum in SideBySide~\cite{willis_sidebyside:_2011}). Tags like QR codes can also be affixed to devices and read by cameras. Common tagging systems are optimized to be read from a close distance, though it is possible to redesign codes that can be read further away (by encoding less information)~\cite{lowcostsensing-uist13}. 
%However, visible tags at appropriate sizes may be rejected by users because of their negative aesthetic effect on the space.

Standard infrared remote controls for televisions and AV equipment tend to use wide-angle LEDs - since they are only meant to control a single device, there is no ambiguity  and it is unnecessary to have the user point precisely.

Commercial universal control applications for smart phones are also available (e.g., Belkin WeMo\footnote{http://www.belkin.com/us/wemo}), though they usually do not offer spatial selection of target devices, forcing users to browse through lists of configured devices instead.

Our main area of differentiation is that we employ head orientation as the selection mechanism instead of pointing - the user looks at the target device. Selection techniques with very small selectors such as laser pointing are less appropriate for head-mounted applications. Head orientation only indicates a general area of visual interest. It does not necessarily match gaze orientation as the eyes can be positioned relative to the head using the extraocular muscles. We therefore select an infrared LED - a source with a wider angle of illumination, but restrict its angle to be narrower than in general purpose IR applications.


\bjoern{Other citations to cover here: Raskar et al's radio frequency identity and geometry~\cite{raskar_rfig_2004}.}

\subsection{Evaluation of room-scale selection}
Pausch et al.'s early investigation of head-mounted displays compared orientation tracking of the head to handheld orientation control for a target acquisition task in a virtual reality room shown on a head-mounted display~\cite{pausch_user_1993}. They found a clear performance benefit for head-mounted tracking.

On the other hand, Card et al. experimentally determined that the bandwidth of neck muscles is much lower than that of arm, wrist or finger muscle groups~\cite{card_morphological_1991}, which limits the performance of any head orientation-based interaction scheme. However, many other factors such as device characteristics and device acquisition time (e.g., pulling a phone out of one's pocket) contribute to overall performance and preference of different selection techniques. Compared to screen where every pixel is a potential target, the required accuracy for physical device selection in a room is much lower.

Myers et al compared different methods of interacting with displays at a distance~\cite{myers_interacting_2002} and quantified selection time and jitter or position error when using laser pointers. Various other techniques outperformed laser pointers.

Rukzio found that users strongly preferred either touching a mobile device to a target appliance to initiate interacting or pointing at a distance to ``scanning'' (initiating a Bluetooth scan and then selecting a target appliance from a list of scan results on the phone )~\cite{rukzio_experimental_2006}.

\subsection{Augmented Reality Interfaces}
\bjoern{most of the prior head-mounted work is in the AR space. I don't know much about it yet, but Kortuem~\cite{kortuem_context-aware_1998} might be a good starting point.}
