%!TEX root = uist14.tex
\section{Introduction}
% \bjoern{something happened to the figure caption font - it doesn't match the standard template. Please fix.}
% \ben{fixed. Problem is because of using caption package}.

%% from the swarm vision to the necessity of selection
The number of smart objects in our environment with embedded computation and communication has grown rapidly. These objects are all potential targets for interaction. To initiate such interaction, a user needs to first acquire the target object -- a fundamental task in an interaction process that has been extensively studied in the digital world, but yet not well-explored in the physical spaces.

%% To initiate such interaction requires {\em selection} information about the object at the user's {\em locus of attention}~\cite{raskin}. \sean{this sentence is unclear to me.}

%% previous approaches are limited
Past research has introduced techniques of augmenting hand-held mobile devices with accessories like laser pointers to enable direct aiming at target devices in space~\cite{beigl_point_1999,patel_2-way_2003}. While promising, some drawbacks of using hand-held devices are that the device first has to be retrieved (e.g., from a pocket) and consciously aimed; that the user's hands are required to be free for operation; and that the user's visual attention is split between looking down at a screen and out at targets in the world. 

%% introducing head-worn computing and head orientation
Emerging head-worn computing devices have the potential to overcome some of these limitations: they do not require retrieval since the devices are already worn; they may enable hands-free or uni-manual interactions; and they offer near-eye or see-through displays to present information adjacent to physical objects in the wearer's field of view. We thus investigate the research question of how such computing devices may be used for selection of physical targets in the space.

Head-worn devices can naturally exploit the user's head orientation, an important, but also a less precise indicator of the user's locus of attention: it contains the general direction, but not the particular point of focus. While gaze tracking could provide further information about a user's focus, wearable gaze tracking cannot by itself reliably determine the identity of objects in focus (e.g. it also needs to know what the user is seeing). Equipping all devices with stationary gaze trackers is possible~\cite{vertegaal2005media} but much costlier than our approach. We therefore study how to leverage head-orientation alone for target selection in physical spaces. 

The imprecision of human head movement suggests adapting area cursor techniques known from assistive devices~\cite{kabbash1995prince,worden1997making,findlater2010enhanced}. Such techniques employ a two-step selection process: a {\em coarse} selection of an area of interest, followed by a {\em refinement} to select a target within that area.

In this paper, we describe the iterative development and evaluation of \systemnamenospace, an area-selection technique that can be readily implemented with small hardware changes to emerging head-worn devices. We augment Google Glass\footnote{\url{http://www.google.com/glass/start/}} to enable infrared (IR) communication between Glass and target appliances. In our {\em Naive IR} technique, the cone of light emitted by an IR LED (a diameter of 30-60cm and distance up to 8m) provides the {\em coarse} selection area (illustrated in Figure~\ref{fig:teaser}A, where targets that have received IR signal light up). To {\em refine} selection when multiple targets have received IR signals, our solution starts with a naive ordering -- based on their names. This is shown in Figure~\ref{fig:teaser}B, where the targets in range are ordered numerically on the near-eye display. A study with $14$ participants compares acquisition times for physical targets in a room for our technique and an alternative list selection interface without any IR targeting \ben{the ordered-list is also list selection, needs clarification!} . We find that target acquisition through head orientation is preferred by users and is faster than list selection but refinements -- if necessary -- are slow.

In response, we designed an improved refinement technique, {\em Intensity IR}, in which target objects compare received IR signal strength. This value allows the system to either eliminate some distracting targets on the peripheral, or to re-order the refinement interface's list by their intensity values. For example, in Figure~\ref{fig:teaser}B of {\em Intensity IR} technique, 5 is eliminated first and the list is re-ordered based on the intensity readings. A second study with $10$ participants proves that {\em Intensity IR} successfully reduces both the chance of needing to do refinement as well as the time spent in list navigation when compared to {\em Naive IR}.

A final design addresses the lack of a natural mapping when users select a target in the refinement step using their device's touchpad - the axes of motion do not map directly to the spatial layout of target devices in a room. Our {\em Head-motion Refinement} technique first learns the relative spatial structure of the targets using Glass' orientation sensors. Then in future target acquisition stage, users can then perform head movements to change selections to spatially adjacent targets (Figure~\ref{fig:teaser}C). For example, nodding down to select the target below current selection, or tilting right to select next target on the right. We present initial feedback on this technique.

%% three ways
%---
We also demonstrate an example application of our technique used as a remote control of smart appliances: a user looks at the appliance he wishes to control and confirms selection by tapping. An appliance-specific user interface is then shown on the user's near-eye display for further interactions.

\ben{I am considering adding back the contribution part, because the paper itself mainly focuses on the iterative design process. Without an explicit contribution statement, the audience will have to infer and summarize themselves.}

%Orientation-based selection enables a wide range of context-aware applications. Examples include smart home remote control, break reminder monitor starer, museum attention tracking, indoor positioning, etc. In Figure\,\ref{fig:teaser}, it's a demonstration of the ``universal remote control'' scenario. The user can easily select the smart appliances by simply looking at it's general direction and confirm such selection with either voice command or by tapping the Glass input pad. Then an appliance-specific control UI will be shown on the head-mounted display. For this application, we have asked 14 participants to try the system and we report the qualitative results from them performing home automation tasks.

%% Contribution
%In summary, this paper makes the following contributions:
%\begin{itemize}
%\item We design and implement a novel head-orientation based selection technique for physical targets based on IR communication. We introduce disambiguation techniques to address the inherent imprecision of head orientation. 
%\item We present evaluations that compare head orientation targeting to list selection and quantify the benefits of automatic disambiguation.
%\item We demonstrate a home appliance remote control application built on top of our selection technique.
%\end{itemize}

%\ben{Having problem fixing the reference}.\bjoern{skip this - we want to be brief.}
%In the remainder of this paper, we will first describe the related works in physical selection and targeting. Given that we focus our scope on head orientation, in Section~\ref{sec:background} we will briefly review human's neck ergonomics as the background for head orientation. In Section~\ref{sec:syst-design-prot}, we then present our system designed for the study of head orientation based selection. The prototype is also what we have used for building the example applications. The disambiguation techniques are discussed in Section~\ref{sec:disamb-techn}, which is followed by the study and evaluation in Section~\ref{sec:evaluation}. To further show the usefulness of having such head orientation-based selection, we describe the enabled applications in Section~\ref{sec:applications}, with detailed implementation about the ``universal remote control'' application. The discussion and conclusion are in Section~\ref{sec:discussion} and Section~\ref{sec:conclusion} respectively. 

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "uist14"
%%% End: 
