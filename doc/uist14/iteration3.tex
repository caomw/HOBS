%!TEX root = uist14.tex
\section{Iteration 3: Head motion refinement}
\label{sec:iteration-3:-head}
\subsection{Problem Statement and Goal}
Both{\em Naive IR} and {\em Intensity IR} rely on list navigation on the near-eye display for the refinement step. This has two problems: First, a spatial layout of target is compressed into a uni-dimensional list. List navigation commands thus have poor spatial mapping. Second, our techniques require the user to switch their focus back and forth between the physical world and the display. This issue motivated us to design an approach that also utilizes orientation information during the {\em refinement} stage, to provide a more direct mapping between the user's actions and target selection.

In our third iteration, we ask the question ``can we build a system purely using head orientation and visual feedback from the environment for target selection?''

\subsection{Technique}
The motion sensors (including gyroscope, magnetometer and accelerometer) on the head-worn device can help us determine the orientation of a user's head. However, this absolute orientation cannot be directly used in any indoor environment as the user may change their location. However, as long as the targets are spread around the periphery, their relative orientation doesn't change. This key observation inspires us to investigate a hybrid solution with IR and orientation sensors on Glass.

There are two phases to this technique: learning the relative orientations or targets in a room and intelligently suggesting targets during disambiguation.

In the first stage, the user orients his head towards each device, and the system saves the absolute orientations of each device. Once the user has added all devices, these orientations are saved for future use. It is still possible to add new devices at any point without running the entire calibration step, as the system will simply store the absolute orientation of the new device.

During the second stage, when the user wants to use his head-motion for disambiguation, they will put their finger on the touchpad to enter a quasi-mode.  This gesture is inspired by observing users who tend to adjust their glasses for closer inspection.

In this mode, only one device has a solid indicator light on at a time. \bjoern{revisit prior sentence - there's a problem in the current technique in that the user doesn't know which targets are in the refinement set and which aren't.} \achal{There is no longer a refinement set - instead the user can select between all the devices using just orientation once they enter quasi mode.} The user can then tilt in the direction of the device they wish to switch to. We calculate the direction of movement using a low-pass-filtered history of orientations, and look for devices that lie in that direction. Once a new device is selected, the frame of reference changes, and the user can repeat the process. Note that in reality, this is a completely smooth operation: it is possible to switch between multiple devices in one continuous motion. This is achieved by automatically adjusting the frame of reference to the currently lit device, but also creating a ``sticky" threshold so that the user is not constantly jumping between devices. \bjoern{This is called hysteresis or double thresholding.}


\bjoern{how well will this work if the user shifts position within a room?}

\achal{Not sure if there should be a conclusion of some sort here.}

\subsection{Evaluation}
\ben{We then need evaluation results here, quantitative and qualitative.}
\bjoern{got to here in my pass}


% %!TEX root = uist14.tex
% \section{Iteration 3: Orientation-based refinement}
% \bjoern{While taking IR intensity into account further reduced the need for manual refinement and increased performance, the UI navigation scheme is problematic - it is not spatially related in any meaningful way to the layout of targets in a room. A better interaction technique would respect that ordering. For example, when refinement is needed, tilting the head slightly to the right could select the target that's adjacent to the right of the currently selected target.

% Such spatial navigation requires knowledge about the layout of targets in the environment. However, one of the strengths of our technique so far is that it does not require any map ahead of time. To enable some spatial navigation, we introduce a final iteration in which we build up a spatial data structure by demonstration (i.e., the user looks around the room) and then leverage that data structure during the refinement step of our interaction.}

% \bjoern{This technique is based on the assumption that users will generally select targets in indoor environments where targets are spread around the periphery. These assumptions enable us to use orientation data without knowing the user's absolute position.}

% \subsection{Implementation}
% \bjoern{give implementation details}

% \subsection{Informal User Feedback}
% \bjoern{We informally evaluate this technique with N users: ...}

 
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "uist14"
%%% End: 
